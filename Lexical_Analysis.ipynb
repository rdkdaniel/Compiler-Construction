{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lexical Analysis",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXbCUtIhF0gCVOFysy+aFY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdkdaniel/Compiler-Construction/blob/main/Lexical_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UGsINj_HusD3"
      },
      "outputs": [],
      "source": [
        "#Simple script breaking words into tokens\n",
        "#Can we generalize this tokenizer?\n",
        "\n",
        "#Key link: https://medium.com/@pythonmembers.club/building-a-lexer-in-python-a-tutorial-3b6de161fe84\n",
        "#Key link: https://www.jayconrod.com/posts/37/a-simple-interpreter-from-scratch-in-python-part-1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#What is a lexer?\n",
        "#From the code written by the user, four major steps follow during compilation\n",
        "#Lexical analysis, parsing, code generation and execution\n",
        "#A Lexer does lexical analysis.\n",
        "#Today, this also includes scanning (before, these two were separate steps)"
      ],
      "metadata": {
        "id": "aGrUdQ0YT38s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lexical analysis turns scanned characters into lexemes (known piece of string)\n",
        "#Refer to the theory material for examples using simple sentences"
      ],
      "metadata": {
        "id": "n7_p2Ea7Uey0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the Lexer**"
      ],
      "metadata": {
        "id": "PHNP_qAfVyJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NB - the lexer will scan a source code and break it into a list of items\n",
        "#Once this is done, it creates a tyoe and value pair ['Integer', '#']\n"
      ],
      "metadata": {
        "id": "VGFZK8C2V45h"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Our Source Code: int result = 20;"
      ],
      "metadata": {
        "id": "Fk_-8uRFWYv4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re                                #Library for performing regex expressions"
      ],
      "metadata": {
        "id": "d1mVegsDWfsi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = []                               # for string tokens\n",
        "source_code = 'int result = 100;'.split() # turning source code into list of words\n",
        "\n",
        "# Loop through each source code word\n",
        "for word in source_code:\n",
        "    \n",
        "    # This will check if a token has datatype decleration\n",
        "    if word in ['str', 'int', 'bool']: \n",
        "        tokens.append(['DATATYPE', word])\n",
        "    \n",
        "    # This will look for an identifier which would be just a word\n",
        "    elif re.match(\"[a-z]\", word) or re.match(\"[A-Z]\", word):\n",
        "        tokens.append(['IDENTIFIER', word])\n",
        "    \n",
        "    # This will look for an operator\n",
        "    elif word in '*-/+%=':\n",
        "        tokens.append(['OPERATOR', word])\n",
        "    \n",
        "    # This will look for integer items and cast them as a number\n",
        "    elif re.match(\".[0-9]\", word):\n",
        "        if word[len(word) - 1] == ';': \n",
        "            tokens.append([\"INTEGER\", word[:-1]])\n",
        "            tokens.append(['END_STATEMENT', ';'])\n",
        "        else: \n",
        "            tokens.append([\"INTEGER\", word])\n",
        "\n",
        "print(tokens) # Outputs the token array"
      ],
      "metadata": {
        "id": "32JppQU3Wwax",
        "outputId": "b7c8d91b-834e-47f6-f20d-c8496691a620",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['DATATYPE', 'int'], ['IDENTIFIER', 'result'], ['OPERATOR', '='], ['INTEGER', '100'], ['END_STATEMENT', ';']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The output above shows that we broke the source code into a token stream\n",
        "#This stream had the 'TYPE' and 'VALUE'"
      ],
      "metadata": {
        "id": "q5lciIqXW6fJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explaining the Code"
      ],
      "metadata": {
        "id": "gIBKV5S7XaGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. First, we imported the regex (regular expression library)\n",
        "    This library was used to check if certain words match certain predefined regex patterns.\n",
        "2. An empty list named \"tokens\" was created [This list would store all the tokens created].\n",
        "3. The source code was split - it was a string broken down into a list of words\n",
        "4. These separated item list was stored in a variable called \"source_code\"\n",
        "5. The first check was then done i.e.\n",
        "    if word in ['str', 'int', 'bool']: \n",
        "       tokens.append(['DATATYPE', word])\n",
        "6. This check verifoed the datatype which then told us the tyoe of variable in use.\n",
        "7. Thereafter, more checks were done, where each word in the source code was identifed and a token was created for it."
      ],
      "metadata": {
        "id": "y5nZ0cp-XhUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "boBOyenBZGIC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}